---
publishDate: 2026-02-17T00:00:00Z
title: 'Evals - Improving AI Code Quality and Consistency'
excerpt: "Your coding agent does not know if the output is good. By implementing a feedback loop you are able to dramatically improve the quality."
image: ~/assets/images/blog/bubble_or_future.png
category: insights
tags:
  - AI
  - Software Development
  - Agentic Engineering
draft: false
---

import CallToAction from '~/components/widgets/CallToAction.astro';
import FreeConsultationCTA from '~/components/ui/FreeConsultationCTA.astro';

From my experience, introducing evals brought the strongest effect on AI generated code.
I will do a deep dive into each of them in the next topics. Here I will briefly run through them to give you the overall idea.

## Static checks

If something can be verified by a deterministic tool, do not rely on a non-deterministic LLM. Run the tool, give the result back to AI.

- Can Agent know if code compiles? - No
- Can it run a build command and see if it compiles? - Yes.

### Build

The easiest way to implement that would be adding this line to your guidelines file: `After any code changes YOU MUST ensure that project builds with no errors`
Simple and straightforward.

### Linters

There are linters for any language and many engineers underestimate their value when working with AI.
For example, my favorite one is cognitive complexity check. It checks how difficult code is to understand.
After code is generated, agent runs linter and if cognitive complexity is above the threshold then it goes back to refactor and simplify the code.
Again, enabling it is simple. `After any code changes YOU MUST use linter and build the project with no errors`.
And that is only a tip of the iceberg.

## Code Review Agents

Imagine you are developing a feature without AI for 10 hours straight. Now you have to do a code review of your code. How effective is it?
It is better to ask your colleague to review with a fresh pair of eyes.

Same thing with AI agents. One agent implements the feature, another one is invoked to review the code.
An agent with fresh context will do a much better review than the agent that did actual coding.
Simple to enable:

```
After any code changes YOU MUST:
1) Run linter and fix errors
2) Run build to ensure it compiles without errors
3) Run agent to review the code you just implemented anf fix errors found
```

As you can see, you can start simple. No hooks, no skills, no specialized agents. Even those 5 lines in your guidelines file will make a difference.
Later you can build on top of that and create agents or skills following specific review rules. But that is later.

## Automated tests

Tests are deterministic. They tell if your system works as expected. Strong teams trust their tests.
If we give this tool to AI then it can be sure that nothing is broken and system behaves as expected.
Or, agent gets instant feedback about something being broken, so instead of shipping broken code to you, it analyzes the reason and fixes it.

Of course it depends a lot on quality of the test suite. But now you have more reasons to invest in the test suite.
In the era of agentic engineering, automated tests are gold.
Honestly, I do not imagine me developing a project without tests. And now they became even cheaper - you can generate them with AI together with features.

I cannot cover tests or features development with AI in this topic because it will become too long.
But if you have a test suite you know what to do:

```
After any code changes YOU MUST:
1) Run linter and fix errors
2) Run build to ensure it compiles without errors
3) Run agent to review the code you just implemented anf fix errors found
4) Run tests to make sure system works as expected (Do not fix tests without my explicit approval)
```

## Summary

Start with five lines in your guidelines file. Add a build check, a linter, a review agent, and tests. You do not need hooks, skills, or custom tooling to begin. These fundamentals compound â€” each eval you add makes the next one more effective. As a bonus, this is your entry into Agentic Engineering.

| Eval Type          | What it catches                        | Deterministic | Effort to enable    |
|--------------------|----------------------------------------|---------------|---------------------|
| Build              | Compilation errors, missing imports    | Yes           | One line            |
| Linters            | Code complexity, style violations      | Yes           | One line            |
| Code Review Agent  | Logic issues, architectural violations | No            | A few lines         |
| Automated Tests    | Regressions, broken behavior           | Yes           | Requires test suite |

---

<CallToAction>
  <Fragment slot="title">Ready to Adopt AI Strategically?</Fragment>

  <Fragment slot="subtitle">
    Let's talk about how to prepare your team for AI adoption. No sales pitch. Just practical advice from someone who's done it.
  </Fragment>

  <Fragment slot="actions">
    <FreeConsultationCTA />
  </Fragment>
</CallToAction>
